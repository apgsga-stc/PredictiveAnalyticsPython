{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def boxplot_histogram(x, bins=None, figsize=(15,10)):\n",
    "    \"\"\"Creates two plots stacked underneath each other. Upper plot: Boxplot. Lower plot: Histogram. Input is any array.\"\"\"\n",
    "    sns.set(style=\"ticks\")\n",
    "    f, (ax_box, ax_hist) = plt.subplots(2, sharex=True, \n",
    "                                        gridspec_kw={\"height_ratios\": (.15, .85)},\n",
    "                                        figsize=figsize)\n",
    "\n",
    "    sns.boxplot(x, notch=True,ax=ax_box)\n",
    "    sns.distplot(x, ax=ax_hist,bins=bins)\n",
    "    ax_hist.grid(True)\n",
    "    ax_hist.set_title('Historgram')\n",
    "    ax_hist.set_ylabel('Percentage')\n",
    "    ax_hist.set_xlabel('Value Range')\n",
    "\n",
    "    ax_box.set(yticks=[])\n",
    "    ax_box.set_title('Boxplot')\n",
    "    ax_box.grid(True)\n",
    "    sns.despine(ax=ax_hist)\n",
    "    sns.despine(ax=ax_box, left=True)\n",
    "    \n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#matplotlib inline\n",
    "import sklearn\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datenaufbereitung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries & Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make imports from pa_lib possible (parent directory of file's directory)\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "file_dir = Path.cwd()\n",
    "parent_dir = file_dir.parent\n",
    "sys.path.append(str(parent_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import qgrid\n",
    "from datetime import datetime as dtt\n",
    "\n",
    "from pa_lib.file import data_files, load_bin, store_bin, load_csv, write_xlsx, load_xlsx\n",
    "from pa_lib.data import (\n",
    "    calc_col_partitioned,\n",
    "    clean_up_categoricals,\n",
    "    unfactorize,\n",
    "    flatten,\n",
    "    replace_col,\n",
    "    cond_col,\n",
    "    desc_col,\n",
    "    unfactorize,\n",
    "    as_dtype,\n",
    "    flatten_multi_index_cols,\n",
    ")\n",
    "from pa_lib.util import obj_size, cap_words, normalize_rows, clear_row_max\n",
    "from pa_lib.log import time_log, info\n",
    "from pa_lib.vis import dive\n",
    "\n",
    "# display long columns completely, show more rows\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "\n",
    "\n",
    "def qshow(df, fit_width=False):\n",
    "    return qgrid.show_grid(\n",
    "        df, grid_options={\"forceFitColumns\": fit_width, \"fullWidthRows\": False}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-17 14:38:39 [INFO] Reading from file C:\\Users\\stc\\data\\bd_data.feather\n",
      "2019-09-17 14:38:40 [INFO] Finished loading binary file in 0.38s (0.73s CPU)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.73 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bd_raw = load_bin(\"bd_data.feather\").rename(\n",
    "    mapper=lambda name: cap_words(name, sep=\"_\"), axis=\"columns\"\n",
    ")\n",
    "bd = bd_raw.loc[(bd_raw.Netto > 0)].pipe(clean_up_categoricals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DTYPE</th>\n",
       "      <th>NULLS</th>\n",
       "      <th>UNIQUE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Endkunde_NR</th>\n",
       "      <td>category</td>\n",
       "      <td>0/675378</td>\n",
       "      <td>32244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Endkunde</th>\n",
       "      <td>category</td>\n",
       "      <td>0/675378</td>\n",
       "      <td>30312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EK_Abc</th>\n",
       "      <td>category</td>\n",
       "      <td>1154/674224</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EK_Boni</th>\n",
       "      <td>category</td>\n",
       "      <td>76/675302</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EK_Plz</th>\n",
       "      <td>category</td>\n",
       "      <td>164/675214</td>\n",
       "      <td>2975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EK_Ort</th>\n",
       "      <td>category</td>\n",
       "      <td>164/675214</td>\n",
       "      <td>3023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EK_Land</th>\n",
       "      <td>category</td>\n",
       "      <td>164/675214</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EK_HB_Apg_Kurzz</th>\n",
       "      <td>category</td>\n",
       "      <td>27734/647644</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EK_Aktiv</th>\n",
       "      <td>category</td>\n",
       "      <td>0/675378</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Agentur</th>\n",
       "      <td>category</td>\n",
       "      <td>322648/352730</td>\n",
       "      <td>3967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AG_Hauptbetreuer</th>\n",
       "      <td>category</td>\n",
       "      <td>324227/351151</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Verkaufsberater</th>\n",
       "      <td>category</td>\n",
       "      <td>0/675378</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Endkunde_Branchengruppe_ID</th>\n",
       "      <td>category</td>\n",
       "      <td>5759/669619</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Endkunde_Branchengruppe</th>\n",
       "      <td>category</td>\n",
       "      <td>5759/669619</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Endkunde_Nbranchengruppe_ID</th>\n",
       "      <td>category</td>\n",
       "      <td>561617/113761</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Endkunde_Nbranchengruppe</th>\n",
       "      <td>category</td>\n",
       "      <td>561617/113761</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Endkunde_Branchenkat_ID</th>\n",
       "      <td>category</td>\n",
       "      <td>5759/669619</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Endkunde_Branchenkat</th>\n",
       "      <td>category</td>\n",
       "      <td>5759/669619</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Endkunde_Nbranchenkat_ID</th>\n",
       "      <td>category</td>\n",
       "      <td>561617/113761</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Endkunde_Nbranchenkat</th>\n",
       "      <td>category</td>\n",
       "      <td>561617/113761</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Auftrag_Branchengruppe_ID</th>\n",
       "      <td>category</td>\n",
       "      <td>0/675378</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Auftrag_Branchengruppe</th>\n",
       "      <td>category</td>\n",
       "      <td>0/675378</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Auftrag_Nbranchengruppe_ID</th>\n",
       "      <td>category</td>\n",
       "      <td>618137/57241</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Auftrag_Nbranchengruppe</th>\n",
       "      <td>category</td>\n",
       "      <td>618137/57241</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Auftrag_Branchenkat_ID</th>\n",
       "      <td>category</td>\n",
       "      <td>0/675378</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Auftrag_Branchenkat</th>\n",
       "      <td>category</td>\n",
       "      <td>0/675378</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Auftrag_Nbranchenkat_ID</th>\n",
       "      <td>category</td>\n",
       "      <td>618153/57225</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Auftrag_Nbranchenkat</th>\n",
       "      <td>category</td>\n",
       "      <td>618153/57225</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Agps_NR</th>\n",
       "      <td>int64</td>\n",
       "      <td>0/675378</td>\n",
       "      <td>675378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Segment</th>\n",
       "      <td>category</td>\n",
       "      <td>0/675378</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KV_NR</th>\n",
       "      <td>int64</td>\n",
       "      <td>0/675378</td>\n",
       "      <td>240309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KV_Typ</th>\n",
       "      <td>category</td>\n",
       "      <td>0/675378</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kampagnen_Status</th>\n",
       "      <td>category</td>\n",
       "      <td>0/675378</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kampagne_Erfassungsdatum</th>\n",
       "      <td>datetime64[ns]</td>\n",
       "      <td>0/675378</td>\n",
       "      <td>3276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kampagne_Beginn</th>\n",
       "      <td>datetime64[ns]</td>\n",
       "      <td>8584/666794</td>\n",
       "      <td>2337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Auftragsart</th>\n",
       "      <td>category</td>\n",
       "      <td>0/675378</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Res_Dat</th>\n",
       "      <td>datetime64[ns]</td>\n",
       "      <td>0/675378</td>\n",
       "      <td>3310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Annullation_Datum</th>\n",
       "      <td>datetime64[ns]</td>\n",
       "      <td>666776/8602</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Aush_Von</th>\n",
       "      <td>datetime64[ns]</td>\n",
       "      <td>0/675378</td>\n",
       "      <td>2646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dauer</th>\n",
       "      <td>int64</td>\n",
       "      <td>0/675378</td>\n",
       "      <td>384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vertrag</th>\n",
       "      <td>category</td>\n",
       "      <td>0/675378</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Brutto</th>\n",
       "      <td>int64</td>\n",
       "      <td>0/675378</td>\n",
       "      <td>38225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Netto</th>\n",
       "      <td>int64</td>\n",
       "      <td>0/675378</td>\n",
       "      <td>32238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Agglo</th>\n",
       "      <td>category</td>\n",
       "      <td>43059/632319</td>\n",
       "      <td>5679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PF</th>\n",
       "      <td>category</td>\n",
       "      <td>636/674742</td>\n",
       "      <td>6343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kamp_Beginn_Jahr</th>\n",
       "      <td>category</td>\n",
       "      <td>8584/666794</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kamp_Beginn_KW</th>\n",
       "      <td>category</td>\n",
       "      <td>8584/666794</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kamp_Beginn_KW_2</th>\n",
       "      <td>category</td>\n",
       "      <td>8584/666794</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kamp_Beginn_KW_4</th>\n",
       "      <td>category</td>\n",
       "      <td>8584/666794</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kamp_Erfass_Jahr</th>\n",
       "      <td>category</td>\n",
       "      <td>0/675378</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kamp_Erfass_KW</th>\n",
       "      <td>category</td>\n",
       "      <td>0/675378</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kamp_Erfass_KW_2</th>\n",
       "      <td>category</td>\n",
       "      <td>0/675378</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kamp_Erfass_KW_4</th>\n",
       "      <td>category</td>\n",
       "      <td>0/675378</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      DTYPE          NULLS  UNIQUE\n",
       "Endkunde_NR                        category       0/675378   32244\n",
       "Endkunde                           category       0/675378   30312\n",
       "EK_Abc                             category    1154/674224       8\n",
       "EK_Boni                            category      76/675302       3\n",
       "EK_Plz                             category     164/675214    2975\n",
       "EK_Ort                             category     164/675214    3023\n",
       "EK_Land                            category     164/675214      46\n",
       "EK_HB_Apg_Kurzz                    category   27734/647644      89\n",
       "EK_Aktiv                           category       0/675378       2\n",
       "Agentur                            category  322648/352730    3967\n",
       "AG_Hauptbetreuer                   category  324227/351151      89\n",
       "Verkaufsberater                    category       0/675378     161\n",
       "Endkunde_Branchengruppe_ID         category    5759/669619      83\n",
       "Endkunde_Branchengruppe            category    5759/669619      83\n",
       "Endkunde_Nbranchengruppe_ID        category  561617/113761      58\n",
       "Endkunde_Nbranchengruppe           category  561617/113761      58\n",
       "Endkunde_Branchenkat_ID            category    5759/669619      22\n",
       "Endkunde_Branchenkat               category    5759/669619      22\n",
       "Endkunde_Nbranchenkat_ID           category  561617/113761      22\n",
       "Endkunde_Nbranchenkat              category  561617/113761      22\n",
       "Auftrag_Branchengruppe_ID          category       0/675378      83\n",
       "Auftrag_Branchengruppe             category       0/675378      83\n",
       "Auftrag_Nbranchengruppe_ID         category   618137/57241      73\n",
       "Auftrag_Nbranchengruppe            category   618137/57241      73\n",
       "Auftrag_Branchenkat_ID             category       0/675378      22\n",
       "Auftrag_Branchenkat                category       0/675378      22\n",
       "Auftrag_Nbranchenkat_ID            category   618153/57225      22\n",
       "Auftrag_Nbranchenkat               category   618153/57225      22\n",
       "Agps_NR                               int64       0/675378  675378\n",
       "Segment                            category       0/675378       8\n",
       "KV_NR                                 int64       0/675378  240309\n",
       "KV_Typ                             category       0/675378       3\n",
       "Kampagnen_Status                   category       0/675378       5\n",
       "Kampagne_Erfassungsdatum     datetime64[ns]       0/675378    3276\n",
       "Kampagne_Beginn              datetime64[ns]    8584/666794    2337\n",
       "Auftragsart                        category       0/675378      12\n",
       "Res_Dat                      datetime64[ns]       0/675378    3310\n",
       "Annullation_Datum            datetime64[ns]    666776/8602     146\n",
       "Aush_Von                     datetime64[ns]       0/675378    2646\n",
       "Dauer                                 int64       0/675378     384\n",
       "Vertrag                            category       0/675378       2\n",
       "Brutto                                int64       0/675378   38225\n",
       "Netto                                 int64       0/675378   32238\n",
       "Agglo                              category   43059/632319    5679\n",
       "PF                                 category     636/674742    6343\n",
       "Kamp_Beginn_Jahr                   category    8584/666794      14\n",
       "Kamp_Beginn_KW                     category    8584/666794      53\n",
       "Kamp_Beginn_KW_2                   category    8584/666794      26\n",
       "Kamp_Beginn_KW_4                   category    8584/666794      13\n",
       "Kamp_Erfass_Jahr                   category       0/675378      13\n",
       "Kamp_Erfass_KW                     category       0/675378      53\n",
       "Kamp_Erfass_KW_2                   category       0/675378      26\n",
       "Kamp_Erfass_KW_4                   category       0/675378      13"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desc_col(bd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kampagnen_Status:\n",
    "- ``1 = In Bearbeitung``\n",
    "- ``2 = Ausgeführt``\n",
    "- ``3 = Annuliert``\n",
    "- ``4 = Abgeschlossen``\n",
    "- ``5 = In Rechnungskorrektur``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-17 14:47:49 [INFO] Finished preparing EK_INFO in 82.45s (83.61s CPU)\n"
     ]
    }
   ],
   "source": [
    "def last_notna(s):\n",
    "    try:\n",
    "        return s.loc[s.last_valid_index()]\n",
    "    except KeyError:\n",
    "        return np.NaN\n",
    "\n",
    "\n",
    "def collect(s, sep=\",\"):\n",
    "    return sep.join(map(str, s[s.notna()].unique()))\n",
    "\n",
    "\n",
    "# this takes around 90 seconds\n",
    "with time_log(\"preparing EK_INFO\"):\n",
    "    ek_info = (\n",
    "        bd.sort_values([\"Endkunde_NR\", \"Kampagne_Erfassungsdatum\"])\n",
    "        .astype({\"Endkunde_NR\": \"int64\", \"Kamp_Erfass_Jahr\": \"int16\"})\n",
    "        .groupby(\"Endkunde_NR\")\n",
    "        .agg(\n",
    "            {\n",
    "                \"Endkunde\": last_notna,\n",
    "                \"EK_Aktiv\": last_notna,\n",
    "                \"EK_Land\": last_notna,\n",
    "                \"EK_Plz\": last_notna,\n",
    "                \"EK_Ort\": last_notna,\n",
    "                \"Agentur\": last_notna,\n",
    "                \"Endkunde_Branchengruppe\": last_notna,\n",
    "                \"Endkunde_Branchengruppe_ID\": last_notna,\n",
    "                \"Auftrag_Branchengruppe_ID\": [collect, \"nunique\"],\n",
    "                \"Kamp_Erfass_Jahr\": [\"min\", \"max\"],\n",
    "                \"Kampagne_Erfassungsdatum\": [\"min\",\"max\"] # max is kind of useless though, since it depends on the respective view date.\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "ek_info.set_axis(\n",
    "    labels=\"\"\"Endkunde EK_Aktiv EK_Land EK_Plz EK_Ort Agentur EK_BG EK_BG_ID Auftrag_BG_ID Auftrag_BG_Anz \n",
    "              Kamp_Erfass_Jahr_min Kamp_Erfass_Jahr_max Kampagne_Erfass_Datum_min Kampagne_Erfass_Datum_max\"\"\".split(),\n",
    "    axis=\"columns\",\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "ek_info = ek_info.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Endkunde_NR</th>\n",
       "      <th>Endkunde</th>\n",
       "      <th>EK_Aktiv</th>\n",
       "      <th>EK_Land</th>\n",
       "      <th>EK_Plz</th>\n",
       "      <th>EK_Ort</th>\n",
       "      <th>Agentur</th>\n",
       "      <th>EK_BG</th>\n",
       "      <th>EK_BG_ID</th>\n",
       "      <th>Auftrag_BG_ID</th>\n",
       "      <th>Auftrag_BG_Anz</th>\n",
       "      <th>Kamp_Erfass_Jahr_min</th>\n",
       "      <th>Kamp_Erfass_Jahr_max</th>\n",
       "      <th>Kampagne_Erfass_Datum_min</th>\n",
       "      <th>Kampagne_Erfass_Datum_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100034</td>\n",
       "      <td>Maurer + Salzmann AG</td>\n",
       "      <td>1</td>\n",
       "      <td>SCHWEIZ</td>\n",
       "      <td>8408</td>\n",
       "      <td>Winterthur</td>\n",
       "      <td>ACE 2 ACE outdoor media ag</td>\n",
       "      <td>WG - Dienstleistung</td>\n",
       "      <td>720</td>\n",
       "      <td>720,405,400,295</td>\n",
       "      <td>4</td>\n",
       "      <td>2009</td>\n",
       "      <td>2018</td>\n",
       "      <td>2009-02-09</td>\n",
       "      <td>2018-10-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100039</td>\n",
       "      <td>Bank BSU Genossenschaft</td>\n",
       "      <td>1</td>\n",
       "      <td>SCHWEIZ</td>\n",
       "      <td>8610</td>\n",
       "      <td>Uster</td>\n",
       "      <td>walder, werber werbeagentur ag</td>\n",
       "      <td>WG - Finanzwirtschaft</td>\n",
       "      <td>470</td>\n",
       "      <td>470,720</td>\n",
       "      <td>2</td>\n",
       "      <td>2009</td>\n",
       "      <td>2019</td>\n",
       "      <td>2009-01-01</td>\n",
       "      <td>2019-09-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100061</td>\n",
       "      <td>Mundwiler Juwelen AG</td>\n",
       "      <td>1</td>\n",
       "      <td>SCHWEIZ</td>\n",
       "      <td>8400</td>\n",
       "      <td>Winterthur</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WG - Persönlicher Bedarf</td>\n",
       "      <td>555</td>\n",
       "      <td>555</td>\n",
       "      <td>1</td>\n",
       "      <td>2009</td>\n",
       "      <td>2009</td>\n",
       "      <td>2009-09-16</td>\n",
       "      <td>2009-09-16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Endkunde_NR                 Endkunde  EK_Aktiv  EK_Land EK_Plz      EK_Ort  \\\n",
       "0       100034     Maurer + Salzmann AG         1  SCHWEIZ   8408  Winterthur   \n",
       "1       100039  Bank BSU Genossenschaft         1  SCHWEIZ   8610       Uster   \n",
       "2       100061     Mundwiler Juwelen AG         1  SCHWEIZ   8400  Winterthur   \n",
       "\n",
       "                          Agentur                     EK_BG EK_BG_ID  \\\n",
       "0      ACE 2 ACE outdoor media ag       WG - Dienstleistung      720   \n",
       "1  walder, werber werbeagentur ag     WG - Finanzwirtschaft      470   \n",
       "2                             NaN  WG - Persönlicher Bedarf      555   \n",
       "\n",
       "     Auftrag_BG_ID  Auftrag_BG_Anz  Kamp_Erfass_Jahr_min  \\\n",
       "0  720,405,400,295               4                  2009   \n",
       "1          470,720               2                  2009   \n",
       "2              555               1                  2009   \n",
       "\n",
       "   Kamp_Erfass_Jahr_max Kampagne_Erfass_Datum_min Kampagne_Erfass_Datum_max  \n",
       "0                  2018                2009-02-09                2018-10-31  \n",
       "1                  2019                2009-01-01                2019-09-01  \n",
       "2                  2009                2009-09-16                2009-09-16  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ek_info.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Booking Data (Beträge: Reservationen & Aushänge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_calc(df, col_year, col_week):\n",
    "    return (\n",
    "        df.loc[:, [\"Endkunde_NR\", col_year, col_week, \"Netto\"]]\n",
    "        .pipe(unfactorize)\n",
    "        .groupby([\"Endkunde_NR\", col_year, col_week], observed=True, as_index=False)\n",
    "        .agg({\"Netto\": [\"sum\"]})\n",
    "        .set_axis(\n",
    "            f\"Endkunde_NR {col_year} {col_week} Netto_Sum\".split(),\n",
    "            axis=\"columns\",\n",
    "            inplace=False,\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def aggregate_bookings(df, period):\n",
    "    info(f\"Period: {period}\")\n",
    "    info(\"Calculate Reservation...\")\n",
    "    df_res = sum_calc(df, \"Kamp_Erfass_Jahr\", f\"Kamp_Erfass_{period}\")\n",
    "    info(\"Calculate Aushang...\")\n",
    "    df_aus = sum_calc(df, \"Kamp_Beginn_Jahr\", f\"Kamp_Beginn_{period}\")\n",
    "\n",
    "    info(\"Merge Results...\")\n",
    "    df_aggr = df_res.merge(\n",
    "        right=df_aus,\n",
    "        left_on=[\"Endkunde_NR\", \"Kamp_Erfass_Jahr\", f\"Kamp_Erfass_{period}\"],\n",
    "        right_on=[\"Endkunde_NR\", \"Kamp_Beginn_Jahr\", f\"Kamp_Beginn_{period}\"],\n",
    "        how=\"outer\",\n",
    "        suffixes=(\"_Res\", \"_Aus\"),\n",
    "    ).rename(\n",
    "        {\"Kamp_Erfass_Jahr\": \"Jahr\", f\"Kamp_Erfass_{period}\": period}, axis=\"columns\"\n",
    "    )\n",
    "\n",
    "    df_aggr = (\n",
    "        df_aggr.fillna(\n",
    "            {\n",
    "                \"Jahr\": df_aggr.Kamp_Beginn_Jahr,\n",
    "                period: df_aggr[f\"Kamp_Beginn_{period}\"],\n",
    "                \"Netto_Sum_Res\": 0,\n",
    "                \"Netto_Sum_Aus\": 0,\n",
    "            }\n",
    "        )\n",
    "        .drop([\"Kamp_Beginn_Jahr\", f\"Kamp_Beginn_{period}\"], axis=\"columns\")\n",
    "        .astype({\"Jahr\": \"int16\"})\n",
    "        .astype({period: \"int8\"})\n",
    "        .sort_values([\"Jahr\", \"Endkunde_NR\", period])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    \n",
    "    # Needed for data preparation\n",
    "    df_aggr.eval(\"YYYYKW_2 = Jahr * 100 + KW_2\", inplace=True)\n",
    "    \n",
    "    return df_aggr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-17 14:51:20 [INFO] Period: KW_2\n",
      "2019-09-17 14:51:20 [INFO] Calculate Reservation...\n",
      "2019-09-17 14:51:21 [INFO] Calculate Aushang...\n",
      "2019-09-17 14:51:21 [INFO] Merge Results...\n"
     ]
    }
   ],
   "source": [
    "bd_aggr_2w = aggregate_bookings(bd, 'KW_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Endkunde_NR</th>\n",
       "      <th>Jahr</th>\n",
       "      <th>KW_2</th>\n",
       "      <th>Netto_Sum_Res</th>\n",
       "      <th>Netto_Sum_Aus</th>\n",
       "      <th>YYYYKW_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103092</td>\n",
       "      <td>2007</td>\n",
       "      <td>45</td>\n",
       "      <td>5310.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>200745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>114140</td>\n",
       "      <td>2007</td>\n",
       "      <td>23</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>200723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>116266</td>\n",
       "      <td>2007</td>\n",
       "      <td>23</td>\n",
       "      <td>5700.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>200723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>142915</td>\n",
       "      <td>2007</td>\n",
       "      <td>13</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>200713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>165220</td>\n",
       "      <td>2007</td>\n",
       "      <td>23</td>\n",
       "      <td>587.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>200723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>177225</td>\n",
       "      <td>2007</td>\n",
       "      <td>45</td>\n",
       "      <td>2365.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>200745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>469638</td>\n",
       "      <td>2007</td>\n",
       "      <td>51</td>\n",
       "      <td>1071.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>200751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>494570</td>\n",
       "      <td>2007</td>\n",
       "      <td>45</td>\n",
       "      <td>5250.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>200745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>494878</td>\n",
       "      <td>2007</td>\n",
       "      <td>47</td>\n",
       "      <td>5900.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>200747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>100098</td>\n",
       "      <td>2008</td>\n",
       "      <td>39</td>\n",
       "      <td>16755.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>200839</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Endkunde_NR  Jahr  KW_2  Netto_Sum_Res  Netto_Sum_Aus  YYYYKW_2\n",
       "0       103092  2007    45         5310.0            0.0    200745\n",
       "1       114140  2007    23        50000.0            0.0    200723\n",
       "2       116266  2007    23         5700.0            0.0    200723\n",
       "3       142915  2007    13        50000.0            0.0    200713\n",
       "4       165220  2007    23          587.0            0.0    200723\n",
       "5       177225  2007    45         2365.0            0.0    200745\n",
       "6       469638  2007    51         1071.0            0.0    200751\n",
       "7       494570  2007    45         5250.0            0.0    200745\n",
       "8       494878  2007    47         5900.0            0.0    200747\n",
       "9       100098  2008    39        16755.0            0.0    200839"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bd_aggr_2w.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'date_now': datetime.datetime(2019, 9, 17, 14, 55, 31, 834383),\n",
       " 'current_year_kw_day': (2019, 38, 2),\n",
       " 'current_yyyykw': 201938,\n",
       " 'number_years': 4,\n",
       " 'date_training': datetime.datetime(2018, 9, 17, 14, 55, 31, 834383)}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datetime as dt\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "date_now      = dt.datetime.now()\n",
    "date_training = date_now - relativedelta(years=1) \n",
    "number_years  = 4 # how many years should be featured\n",
    "\n",
    "current_year_kw_day = date_now.isocalendar()\n",
    "current_yyyykw = current_year_kw_day[0]*100+current_year_kw_day[1] # Current calender week in format: YYYYKW\n",
    "\n",
    "global_variables = dict({\"date_now\": date_now, \n",
    "                         \"current_year_kw_day\": current_year_kw_day,\n",
    "                         \"current_yyyykw\"     : current_yyyykw,\n",
    "                         \"number_years\"       : number_years,\n",
    "                         \"date_training\"      : date_training}\n",
    "                       )\n",
    "display(global_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep: Booking Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def booking_yearly_totals(YYYYKW, year_span):\n",
    "    \"\"\"\n",
    "    Computing yearly totals for Aushang and Reservation.\n",
    "    Warning: Yearly totals do not necessarily align with calendar years!\n",
    "    \"\"\"\n",
    "    #info(\"Starting: booking_yearly_totals\")\n",
    "    container_df = pd.DataFrame()\n",
    "    container_df.loc[:,\"Endkunde_NR\"] =pd.Series(list(set(bd_aggr_2w.loc[:,\"Endkunde_NR\"])))\n",
    "    #bd_aggr_2w.eval(\"YYYYKW_2 = Jahr * 100 + KW_2\", inplace=True)\n",
    "    info(\"Computing: Yearly total sums\")\n",
    "    for ry in list(range(year_span)):\n",
    "        #bd_aggr_2w.loc[:,\"YYYYKW_2\"] = bd_aggr_2w.Jahr.map(lambda x: x*100) + bd_aggr_2w.KW_2\n",
    "                \n",
    "        bd_filtered = bd_aggr_2w.loc[((bd_aggr_2w.loc[:,\"YYYYKW_2\"] <  YYYYKW-100*ry) &\n",
    "                                      (bd_aggr_2w.loc[:,\"YYYYKW_2\"] >= YYYYKW-100*(1+ry))),:].copy()\n",
    "        \n",
    "        bd_filtered.loc[:,\"Year_Total\"] = \"_RY_\"+str(ry)\n",
    "        \n",
    "        bd_pivot = bd_filtered.pivot_table(\n",
    "            #index=[\"Endkunde_NR\", \"Jahr\"],\n",
    "            index=[\"Endkunde_NR\"],\n",
    "            #columns=\"KW_2\",\n",
    "            columns = [\"Year_Total\"],\n",
    "            values=[\"Netto_Sum_Res\",\"Netto_Sum_Aus\"], # Cash amount of Resevation placed per in weeks of YYYYKW and YYYY(KW+1)\n",
    "            aggfunc=\"sum\",\n",
    "            fill_value=0, # There's a difference between 0 and NaN. Consider 0 only when the customer has had a real booking or reservation prior.\n",
    "        )\n",
    "        \n",
    "        # Flatten down dataframe:\n",
    "        bd_flattened = pd.DataFrame(bd_pivot.to_records(index=False))\n",
    "        \n",
    "        # Re-add column with Endkunde_NR\n",
    "        bd_flattened.loc[:,\"Endkunde_NR\"] = pd.Series(bd_pivot.index)\n",
    "        \n",
    "        # Renaming column names:\n",
    "        bd_flattened.columns = [x.replace(\"', '\",'')\n",
    "                                .replace(\"', \",'')\n",
    "                                .replace(\"('\",\"\")\n",
    "                                .replace(\")\",\"\")\n",
    "                                .replace(\"'\",\"\") for x in bd_flattened.columns]\n",
    "        \n",
    "        # Left-Join to the container\n",
    "        info(\"Merging: Left-Join to Container dataframe\")\n",
    "        container_df = pd.merge(container_df, bd_flattened, on=\"Endkunde_NR\", how=\"left\")\n",
    "    \n",
    "    # Replace all NaN with Zero\n",
    "    container_df.fillna(0, inplace=True)\n",
    "    \n",
    "    return container_df\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def booking_data(YYYYKW, year_span):\n",
    "    \"\"\"\n",
    "    Creates pivot table for time span between YYYYKW and back the selected amount of years year_span\n",
    "    \"\"\"\n",
    "    # Select the last four years based on new reference-column\n",
    "    bd_filtered = bd_aggr_2w.loc[((bd_aggr_2w.loc[:,\"YYYYKW_2\"] <= YYYYKW) &\n",
    "                                  (bd_aggr_2w.loc[:,\"YYYYKW_2\"] >=  YYYYKW-year_span*100)),:].copy()\n",
    "    \n",
    "    # Create new column containing names of the relative years: \n",
    "    #pd.options.mode.chained_assignment = None  # default='warn'\n",
    "    max_Jahr = YYYYKW//100\n",
    "    bd_filtered.loc[:,\"Jahr_relative\"] = \"_RY_\"+(max_Jahr-bd_filtered.loc[:,\"Jahr\"]).astype('str')+\"_KW_\"\n",
    "    #pd.options.mode.chained_assignment = 'warn'  # default='warn'\n",
    "    \n",
    "    # Computing Sums for each KW and customer\n",
    "    info(\"Computing: Pivot Table\")\n",
    "    bd_pivot    = bd_filtered.pivot_table(\n",
    "        #index=[\"Endkunde_NR\", \"Jahr\"],\n",
    "        index=[\"Endkunde_NR\"],\n",
    "        #columns=\"KW_2\",\n",
    "        columns = [\"Jahr_relative\",\"KW_2\"],\n",
    "        values=[\"Netto_Sum_Res\",\"Netto_Sum_Aus\"] , # Cash amount of Resevation placed per in weeks of YYYYKW and YYYY(KW+1)\n",
    "        aggfunc=\"sum\",\n",
    "        fill_value=0, # There's a difference between 0 and NaN. Consider 0 only when the customer has had a real booking or reservation prior.\n",
    "    )\n",
    "    # Flatten down dataframe\n",
    "    bd_flattened = pd.DataFrame(bd_pivot.to_records(index=False))\n",
    "    \n",
    "    # Read column with Endkunde\n",
    "    bd_flattened.loc[:,\"Endkunde_NR\"] = pd.Series(bd_pivot.index)\n",
    "    \n",
    "    # Renaming column names:\n",
    "    bd_flattened.columns = [x.replace(\"', '\",'')\n",
    "                            .replace(\"', \",'')\n",
    "                            .replace(\"('\",\"\")\n",
    "                            .replace(\")\",\"\") for x in bd_flattened.columns]\n",
    "    \n",
    "    # Label target variables:\n",
    "    KW = \"KW_\"+str(int(YYYYKW- np.floor(YYYYKW/100)*100))\n",
    "    bd_flattened.rename(columns={\"Netto_Sum_Res_RY_0_\"+KW: \"Target_Sum_Res_RY_0_\"+KW,\n",
    "                                 \"Netto_Sum_Aus_RY_0_\"+KW: \"Target_Sum_Aus_RY_0_\"+KW},\n",
    "                        inplace=True)\n",
    "    bd_flattened.loc[:,\"Target_Res_flg\"] = bd_flattened.loc[:,\"Target_Sum_Res_RY_0_\"+KW].astype('bool') # Reservation?: Yes/No - True/False\n",
    "    bd_flattened.loc[:,\"Target_Aus_flg\"] = bd_flattened.loc[:,\"Target_Sum_Aus_RY_0_\"+KW].astype('bool') # Aushang?: Yes/No - True/False\n",
    "    \n",
    "    # Sort index\n",
    "    bd_flattened.sort_index(axis=1, inplace=True)\n",
    "    \n",
    "    # Compute yearly totals\n",
    "    info(\"Running: booking_yearly_totals(YYYYKW, year_span) \")\n",
    "    yearly_totals = booking_yearly_totals(YYYYKW, year_span)\n",
    "    \n",
    "    # Left join yearly totals, and return it\n",
    "    info(\"Final merge\")\n",
    "    return pd.merge(bd_flattened, yearly_totals, on=\"Endkunde_NR\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-17 14:56:39 [INFO] Computing: Pivot Table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating: scoring_bd\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Target_Sum_Res_RY_0_KW_38'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2656\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2657\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2658\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Target_Sum_Res_RY_0_KW_38'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-2e57a4187454>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Creating: scoring_bd\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mscoring_bd\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mbooking_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_yyyykw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Creating: training_bd\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtraining_bd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbooking_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_yyyykw\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-17c3a4b4acb7>\u001b[0m in \u001b[0;36mbooking_data\u001b[1;34m(YYYYKW, year_span)\u001b[0m\n\u001b[0;32m     41\u001b[0m                                  \"Netto_Sum_Aus_RY_0_\"+KW: \"Target_Sum_Aus_RY_0_\"+KW},\n\u001b[0;32m     42\u001b[0m                         inplace=True)\n\u001b[1;32m---> 43\u001b[1;33m     \u001b[0mbd_flattened\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Target_Res_flg\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbd_flattened\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Target_Sum_Res_RY_0_\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mKW\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'bool'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Reservation?: Yes/No - True/False\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m     \u001b[0mbd_flattened\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Target_Aus_flg\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbd_flattened\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Target_Sum_Aus_RY_0_\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mKW\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'bool'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Aushang?: Yes/No - True/False\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1492\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1493\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1494\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1495\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1496\u001b[0m             \u001b[1;31m# we by definition only have the 0th axis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m    866\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    867\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 868\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_lowerdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    869\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m             \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_lowerdim\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m    986\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_label_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 988\u001b[1;33m                 \u001b[0msection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    989\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    990\u001b[0m                 \u001b[1;31m# we have yielded a scalar ?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1911\u001b[0m         \u001b[1;31m# fall thru to straight lookup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1912\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1913\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1914\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1915\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_label\u001b[1;34m(self, label, axis)\u001b[0m\n\u001b[0;32m    139\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'no slices here, handle elsewhere'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_xs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mxs\u001b[1;34m(self, key, axis, level, drop_level)\u001b[0m\n\u001b[0;32m   3574\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3575\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3576\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3577\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3578\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2925\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2926\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2927\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2928\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2929\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2657\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2658\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2659\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2660\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2661\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Target_Sum_Res_RY_0_KW_38'"
     ]
    }
   ],
   "source": [
    "print(\"Creating: scoring_bd\")\n",
    "scoring_bd  = booking_data(current_yyyykw,4)\n",
    "\n",
    "print(\"Creating: training_bd\")\n",
    "training_bd = booking_data(current_yyyykw-100,4)\n",
    "\n",
    "# Check if both tables have the same columns names\n",
    "print(\"[\", list(scoring_bd.columns) == list(training_bd.columns), \"] Both sets have same columns\")\n",
    "\n",
    "# Show me the first few lines\n",
    "print(\"training_bd:\")\n",
    "training_bd.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_colnames_bd = list(training_bd.columns)   \n",
    "# Don't scale the following columns:\n",
    "feature_colnames_bd.remove(\"Endkunde_NR\")\n",
    "feature_colnames_bd.remove(\"Target_Aus_flg\")\n",
    "feature_colnames_bd.remove(\"Target_Res_flg\")\n",
    "display(feature_colnames_bd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reservation Dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Erste Reservation muss vor _View Date_ liegen\n",
    "2. Die letzte Reservation vor dem _View Date_ darf nicht in den letzten 2 Wochen liegen. Sehr unwahrscheinlich, dass diese gleich nochmal buchen, ausserdem  technische Vermeidung von Überlappungen\n",
    "3. Muss Target Aussage haben. (Zur zeit Reservationen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dates_bd(view_date):\n",
    "    sec_kw2_factor = (60*60*24*365)\n",
    "    min_max_erfass_dt = (\n",
    "        bd.loc[view_date>bd.loc[:,\"Kampagne_Erfassungsdatum\"],[\"Endkunde_NR\", \"Kampagne_Erfassungsdatum\"]]\n",
    "          .groupby(\"Endkunde_NR\")\n",
    "          .agg(['min','max'])\n",
    "          ).reset_index()\n",
    "\n",
    "    min_max_erfass_dt = pd.DataFrame(min_max_erfass_dt.to_records(index=False))\n",
    "\n",
    "    min_max_erfass_dt.columns = [\"Endkunde_NR\",\n",
    "                                 \"Kampagne_Erfass_Datum_min\",\n",
    "                                 \"Kampagne_Erfass_Datum_max\"]\n",
    "\n",
    "    min_max_erfass_dt.loc[:,\"Erste_Buchung_Delta\"] = (\n",
    "        min_max_erfass_dt\n",
    "        .loc[:,\"Kampagne_Erfass_Datum_min\"]\n",
    "        .apply(lambda x: ((view_date-x).total_seconds()) // sec_kw2_factor)\n",
    "        #.fillna(-1)\n",
    "        )\n",
    "\n",
    "    min_max_erfass_dt.loc[:,\"Letzte_Buchung_Delta\"] = (\n",
    "        min_max_erfass_dt\n",
    "        .loc[:,\"Kampagne_Erfass_Datum_max\"]\n",
    "        .apply(lambda x: (view_date-x).total_seconds() // sec_kw2_factor)\n",
    "        #.fillna(-1)\n",
    "        )\n",
    "    \n",
    "    min_max_erfass_dt.loc[:,\"Erste_Letzte_Buchung_Delta\"] = (\n",
    "        min_max_erfass_dt.loc[:,\"Erste_Buchung_Delta\"] - min_max_erfass_dt.loc[:,\"Letzte_Buchung_Delta\"]\n",
    "        )\n",
    "    # Kick all customer, who just booked in the last two weeks\n",
    "    final_selection = (\n",
    "        min_max_erfass_dt\n",
    "        .loc[min_max_erfass_dt\n",
    "             .loc[:,\"Kampagne_Erfass_Datum_max\"]\n",
    "             .apply(lambda x: x + relativedelta(weeks=2) < view_date),:])\n",
    "    \n",
    "    return final_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating: training_dates\")\n",
    "training_dates = dates_bd(date_training)\n",
    "print(\"Creating: scoring_dates\")\n",
    "scoring_dates  = dates_bd(date_now)\n",
    "\n",
    "# Check if both tables have the same columns names\n",
    "print(\"[\", list(scoring_dates.columns) == list(training_dates.columns), \"] Both sets have same columns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(training_dates.describe())\n",
    "display(desc_col(training_dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dates.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store feature names in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_colnames_dates = list(training_dates.columns)\n",
    "feature_colnames_dates.remove(\"Endkunde_NR\")\n",
    "feature_colnames_dates.remove(\"Kampagne_Erfass_Datum_min\")\n",
    "feature_colnames_dates.remove(\"Kampagne_Erfass_Datum_max\")\n",
    "print(feature_colnames_dates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Reservation Dates\n",
    "2. Booking data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Remark:</b> Merge via INNER-JOIN, to apply all necessary filtration criteria.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_all = pd.merge(training_dates,training_bd,on=\"Endkunde_NR\", how=\"inner\")\n",
    "scoring_all = pd.merge(scoring_dates,  scoring_bd, on=\"Endkunde_NR\", how=\"inner\")\n",
    "\n",
    "display(training_all.head(3))\n",
    "\n",
    "# Check if both tables have the same columns names\n",
    "print(\"[\", list(scoring_all.columns) == list(training_all.columns), \"] Both sets have same columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = training_all\n",
    "print(\"Target_Res_flg == False\")\n",
    "boxplot_histogram(temp_df.loc[ temp_df.Target_Res_flg == False ,\"Erste_Buchung_Delta\"])\n",
    "print(\"Target_Res_flg == True\")\n",
    "boxplot_histogram(temp_df.loc[ temp_df.Target_Res_flg == True ,\"Erste_Buchung_Delta\"])\n",
    "\n",
    "print(\"Target_Res_flg == False\")\n",
    "boxplot_histogram(temp_df.loc[ temp_df.Target_Res_flg == False ,\"Letzte_Buchung_Delta\"])\n",
    "print(\"Target_Res_flg == True\")\n",
    "boxplot_histogram(temp_df.loc[ temp_df.Target_Res_flg == True ,\"Letzte_Buchung_Delta\"])\n",
    "\n",
    "\n",
    "print(\"Target_Res_flg == False\")\n",
    "boxplot_histogram(temp_df.loc[ temp_df.Target_Res_flg == False ,\"Erste_Letzte_Buchung_Delta\"])\n",
    "print(\"Target_Res_flg == True\")\n",
    "boxplot_histogram(temp_df.loc[ temp_df.Target_Res_flg == True ,\"Erste_Letzte_Buchung_Delta\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Remark:</b> Scaling has to take place after all filtrations have taken place!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling_bd(dataset,col_bookings=[], col_dates=[]):\n",
    "    \"\"\"\n",
    "    Booking columns are heavily right-skewed:\n",
    "     1. log-transform all columns => achieving approx. gaussian distribution\n",
    "     2. Standardise log-transformed values into interval [0,1]\n",
    "     \n",
    "    Return transformed dataframe\n",
    "    \"\"\"\n",
    "    # Scaling: booking\n",
    "    for x in col_bookings:\n",
    "        logtransformed = np.log(dataset.loc[:,x]+1) # bookings are heavily right-skewed. log-transform to get approx. gaussian distribution\n",
    "        min_ = np.min(logtransformed)\n",
    "        max_ = np.max(logtransformed)\n",
    "        dataset[x] = (logtransformed-min_)/(max_-min_) # standardise into floats in [0,1]\n",
    "    \n",
    "    for x in col_dates:\n",
    "        transformed = dataset.loc[:,x]\n",
    "        min_ = np.min(transformed)\n",
    "        max_ = np.max(transformed)\n",
    "        dataset[x] = (transformed-min_)/ (max_-min_)  \n",
    "        \n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_training_all = scaling_bd(training_all,col_bookings=feature_colnames_bd, col_dates=feature_colnames_dates)\n",
    "scaled_scoring_all  = scaling_bd(scoring_all, col_bookings=feature_colnames_bd, col_dates=feature_colnames_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "scaled_training_all.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(list(training_all.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before:\")\n",
    "boxplot_histogram(       training_all.loc[:,\"Netto_Sum_Aus_RY_1\"],bins=20)\n",
    "print(\"After:\")\n",
    "boxplot_histogram(scaled_training_all.loc[:,\"Netto_Sum_Aus_RY_1\"],bins=20)\n",
    "print(\"Before:\")\n",
    "boxplot_histogram(       training_all.loc[:,\"Netto_Sum_Aus_RY_2\"],bins=20)\n",
    "print(\"After:\")\n",
    "boxplot_histogram(scaled_training_all.loc[:,\"Netto_Sum_Aus_RY_2\"],bins=20)\n",
    "print(\"Before:\")\n",
    "boxplot_histogram(       training_all.loc[:,\"Netto_Sum_Aus_RY_3\"],bins=20)\n",
    "print(\"After:\")\n",
    "boxplot_histogram(scaled_training_all.loc[:,\"Netto_Sum_Aus_RY_3\"],bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Copy for R-Studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_training_all.to_csv(\"C:\\\\Users\\\\stc\\\\data\\\\blahblah.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRM data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ct_raw = load_bin(\"crm_data_vkprog.feather\").rename(\n",
    "    mapper=lambda name: cap_words(name, sep=\"_\"), axis=\"columns\"\n",
    ")\n",
    "ct = ct_raw.pipe(clean_up_categoricals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_col(ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level1 = pd.Series(ct.Kanal).isin([\"E-Mail\", \"Telefon\"])\n",
    "level2 = pd.Series(ct.Kanal).isin([\"Besprechung\", \"Besuch\"])\n",
    "\n",
    "ct[\"level1\"] = level1.map(lambda x: int(x))\n",
    "ct[\"level2\"] = level2.map(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct.groupby(\"Kanal\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct[(ct[\"level1\"] == 1) | (ct[\"level2\"] == 1)].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_flat = ct[(ct[\"level1\"] > 0) | (ct[\"level2\"] > 0)].pivot_table(\n",
    "    #index=[\"Endkunde_NR\", \"Jahr\"],\n",
    "    index=[\"Endkunde_NR\"],\n",
    "    #columns=\"KW_2\",\n",
    "    columns = [\"Year\",\"KW_2\"],\n",
    "    values=[\"level1\",\"level2\"] ,\n",
    "    aggfunc=\"max\",\n",
    "    fill_value=0, # There's a difference between 0 and NaN. Consider 0 only when the customer has had a real booking or reservation prior.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_flat.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_col(ct_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(ct_flat.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_flattened = pd.DataFrame(ct_flat.to_records(index=False))\n",
    "ct_flattened[\"Endkunde_NR\"] = pd.Series(ct_flat.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% flatten booking dataset\n",
    "ct_flattened.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Clean up column names\n",
    "raw_col_names_ct = list(ct_flattened.columns)[0:-1]\n",
    "splitted_col_names = [x.lstrip(\"(\").rstrip(\")\").split(\", \") for x in raw_col_names_ct]\n",
    "col_names_flattened = [(content_name.replace(\"'\",\"\"), int(x)*100+int(y)) for [content_name,x,y] in splitted_col_names] + list(ct_flattened.columns)[ct_flattened.shape[1]-1:ct_flattened.shape[1]]\n",
    "\n",
    "#%% Set cleaned up column names\n",
    "ct_flattened.columns = col_names_flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_flattened.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ## ## ## ## ## ## ## ## ## ## ##\n",
    "## ## ## ## ## ## ## ## ## ## ## ##\n",
    "## ## ## ## ## ## ## ## ## ## ## ##\n",
    "## ## ## ## ## ## ## ## ## ## ## ##\n",
    "## ## ## ## ## ## ## ## ## ## ## ##\n",
    "## ## ## ## ## ## ## ## ## ## ## ##\n",
    "## ## ## ## ## ## ## ## ## ## ## ##\n",
    "## ## ## ## ## ## ## ## ## ## ## ##\n",
    "## ## ## ## ## ## ## ## ## ## ## ##\n",
    "## ## ## ## ## ## ## ## ## ## ## ##\n",
    "## ## ## ## ## ## ## ## ## ## ## ##\n",
    "## ## ## ## ## ## ## ## ## ## ## ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data: tds == TDS == gv_FILE_OUT_TDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TDS = pd.read_csv(\"C:/Users/stc/Documents/notebooks/Verkaufsprognose Modeling/TDS.csv\", low_memory=False, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(TDS.describe())\n",
    "print(TDS.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TDS.head(5)\n",
    "#DS.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermezzo: Whiteboard problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which customers have the most rows in the dataframe?\n",
    "(TDS.groupby([\"ENDKUNDE_NR\"]).size().to_frame('COUNT')\n",
    "                             .reset_index()\n",
    "                             .sort_values(['COUNT', 'ENDKUNDE_NR'],ascending=[False,False])\n",
    "                             .head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TDS.loc[TDS.ENDKUNDE_NR == 606399, # row selection\n",
    "        :\n",
    "        \n",
    "       ]         # column selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TDS.loc[TDS.ENDKUNDE_NR == 606399, # row selection\n",
    "        [\"ENDKUNDE_NR\", \"AJ\",\n",
    "        \"AJ_27\",\"VJ_27\",\"VVJ_27\",\"VVVJ_27\",\n",
    "         \"AJ_31\",\"VJ_31\",\"VVJ_31\",\"VVVJ_31\",\n",
    "         \"AJ_35\",\"VJ_35\",\"VVJ_35\",\"VVVJ_35\"\n",
    "        ]\n",
    "        \n",
    "       ]         # column selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TDS.loc[TDS.ENDKUNDE_NR == 608966, # row selection\n",
    "        [\"ENDKUNDE_NR\", \"AJ\",\n",
    "         \"AJ_03\",\"VJ_03\", \"VVJ_03\",\"VVVJ_03\",  \"AJ_51\",\"VJ_51\",\"VVJ_51\",\"VVVJ_51\"]\n",
    "        \n",
    "       ]         # column selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TDS.loc[TDS.ENDKUNDE_NR == 608966, # row selection\n",
    "        [\"ENDKUNDE_NR\", \"AJ\",\n",
    "         \"AJ_03\",\"VJ_03\", \"VVJ_03\",\"VVVJ_03\",  \"AJ_51\",\"VJ_51\",\"VVJ_51\",\"VVVJ_51\"]\n",
    "        \n",
    "       ]         # column selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Originale erhalten für Vorhersage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gv_AJAW_KW = 29 # aktuelle Kalenderwoche, basierend auf gv_AJAW = Datum an dem das Script lief, 15.07.2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns   = [] # Features/Merkmale\n",
    "predictor_columns = [] # Predictores/Targets\n",
    "leftover_columns  = [] # Leftover\n",
    "for col_name in TDS.keys():\n",
    "    if (col_name.startswith(\"VJ\")  | # Umsatz Vorjahr\n",
    "        col_name.startswith(\"VVJ\") |\n",
    "        col_name.startswith(\"VVVJ\")|\n",
    "        col_name.startswith(\"B\")   ):\n",
    "        feature_columns.append(col_name)\n",
    "    elif (col_name.startswith(\"AJ_\") and len(col_name) < 6)  :\n",
    "        predictor_columns.append(col_name)\n",
    "    else:\n",
    "        leftover_columns.append(col_name)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exclude ``VJ_KONTAKT_L1_KW29``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns.remove(f\"VJ_KONTAKT_L1_KW{gv_AJAW_KW}\") # it's removed in the R-Markdown script\n",
    "#feature_columns.remove(\"VJNet\")\n",
    "feature_columns.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_describe = TDS.loc[:,feature_columns+[\"AJ_29\"]]\n",
    "df_describe[\"target\"] = TDS[\"AJ_29\"].map(lambda x: int(bool(abs(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def desc_tool(df_x):\n",
    "    pd.options.display.max_columns = None\n",
    "    display(df_x.head(5))\n",
    "    display(df_x.describe())\n",
    "    #print(\"\\ndf_x.info():\",df_x.info())\n",
    "    print(\"\\nDataset Shape:\",df_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_tool(df_describe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#pairplot\n",
    "sns.pairplot(df_describe.loc[:,[\"VJ_NETKAT_01\",\"VJNet\",\"target\"]], hue=\"target\", height=3)\n",
    "plt.savefig(\"pairplot.png\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.FacetGrid(df_describe, hue=\"target\", height=4).map(sns.distplot,\"VJNet\").add_legend()\n",
    "plt.show()\n",
    "fig, ax = plt.subplots()\n",
    "ax.set(yscale=\"log\")\n",
    "sns.boxplot(x=\"target\",y=\"VJNet\", data=df_describe, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.mlab as mlab\n",
    "from   matplotlib.colors import Normalize\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = TDS.loc[TDS.AJ_29>0 ,\"AJ_29\"]\n",
    "y = np.log(TDS.loc[TDS.AJ_29>0,\"AJ_29\"]+1)\n",
    "n_bins=50\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, sharey=True, tight_layout=True, figsize=(15,5))\n",
    "\n",
    "# We can set the number of bins with the `bins` kwarg\n",
    "axs[0].hist(x, bins=n_bins)\n",
    "axs[0].grid(True)\n",
    "axs[1].hist(y, bins=n_bins)\n",
    "axs[1].grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TDS.loc[TDS.AJ_29>0,\"AJ_29\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset-Mapping to Zero/One"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediktor festlegen: Aktuelle Woche basierend auf ``gv_AJAW_KW = 29``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ft_binary = TDS.fillna(0).loc[:,feature_columns+[\"AJ_29\"]]\n",
    "for col in df_ft_binary.columns:\n",
    "    df_ft_binary[col] = df_ft_binary[col].map(lambda x: int(bool(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=> ``df_ft_binary``: Dataframe with features and target, all entries as binaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"All occuring values in df_ft_binary are binary:\",all(df_ft_binary.isin([0,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split: Training & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = df_ft_binary.loc[:,feature_columns].to_numpy()\n",
    "df_target   = df_ft_binary.loc[:, \"AJ_29\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_features    = TDS.fillna(0).loc[:,feature_columns].to_numpy()\n",
    "#df_target      = TDS.fillna(0).loc[:,\"AJ_29\"].to_numpy()\n",
    "\n",
    "print(\"Data type:\")\n",
    "print(\"df_features:\", type(df_features))\n",
    "print(\"df_target:\", type(df_target))\n",
    "print(\"\\nShape:\")\n",
    "print(\"df_features:\", df_features.shape)\n",
    "print(\"df_target:\", df_target.shape)\n",
    "print(\"\\nColumn Names:\")\n",
    "print(\"Features:\\n\", feature_columns)\n",
    "print(\"Target:\", \"AJ_29\") # <= NEEDS TO BE ADJUSTED!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    #df_features, df_target, train_size=0.80, random_state=42)\n",
    "    df_features, df_target, train_size=0.75, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape:\")\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print(\"y_test:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "print('y_train:')\n",
    "print(pd.DataFrame(y_train).groupby(0)[0].count())\n",
    "print(stats.describe(y_train))\n",
    "\n",
    "print('\\ny_test:')\n",
    "print(pd.DataFrame(y_test).groupby(0)[0].count())\n",
    "print(stats.describe(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Remark:</b> Dataset is massively imbalanced! Only roughly 3% of the Training dataset is 1. This means if I predict all customers to have a Zero, I will be in 97% of the cases correct (on the training set). In order to achieve more reliable scoring models, the training dataset (X_train, y_train) has to be balanced.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### Balance Training Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synthetic Minority Over-sampling Technique (SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install imblearn\n",
    "#from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "#ros = RandomOverSampler(random_state=42)\n",
    "sm  = SMOTE(random_state=42)\n",
    "\n",
    "X_train_balanced, y_train_balanced = sm.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('y_train_balanced:')\n",
    "print(pd.DataFrame(y_train_balanced).groupby(0)[0].count())\n",
    "print(stats.describe(y_train_balanced))\n",
    "\n",
    "pd.Series(y_train_balanced).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C is the Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization.\n",
    "\n",
    "1. logreg:   $C = 1$\n",
    "2. logreg01: $C = 0.1$\n",
    "3. logreg00001: $C = 0.0001$\n",
    "4. logreg100: $C = 100$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_logreg(C=[1]):\n",
    "    \"\"\"Create different models for varying C parameter. Put everything into a dictionary.\"\"\"\n",
    "    logreg_dict = {}\n",
    "    for x in C:\n",
    "        logreg_dict[x] = LogisticRegression(n_jobs=-1,\n",
    "                                    solver=\"sag\",\n",
    "                                    C=x, # to be adjusted\n",
    "                                    max_iter=1000000\n",
    "                                   ).fit(X_train_balanced, y_train_balanced)\n",
    "    return logreg_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_dict = create_logreg([1,0.0001,0.01,0.1,100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_scores = pd.DataFrame([\n",
    "    [x, logreg_dict[x].score(X_train_balanced, y_train_balanced),\n",
    "        logreg_dict[x].score(X_test, y_test)] for x in logreg_dict.keys()], columns=[\"LogReg C\", \"Training set score\",\"Test Set score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "logreg = LogisticRegression(n_jobs=-1,\n",
    "                            max_iter=100000,\n",
    "                            solver=\"sag\",\n",
    "                            C=1000 # to be adjusted\n",
    "                           ).fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "#logreg = LogisticRegression().fit(X_train, y_train)\n",
    "print(\"Training set score: {:.3f}\".format(logreg.score(X_train_balanced, y_train_balanced)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# %% Train data: LogReg with C = 0.01\n",
    "logreg001 = LogisticRegression(n_jobs=-1,\n",
    "                               max_iter=100000,\n",
    "                               solver=\"sag\", # L2 regularisation\n",
    "                               C=0.01 # adjustable\n",
    "                              ).fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "print(\"Training set score: {:.3f}\".format(logreg001.score(X_train_balanced, y_train_balanced)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg001.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "logreg00001 = LogisticRegression(n_jobs=-1,\n",
    "                                 max_iter=100000,\n",
    "                               solver=\"sag\",\n",
    "                               C=0.0001 # adjustable\n",
    "                              ).fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "print(\"Training set score: {:.3f}\".format(logreg00001.score(X_train_balanced, y_train_balanced)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg00001.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "logreg100 = LogisticRegression(n_jobs=-1,\n",
    "                               solver=\"sag\",\n",
    "                               max_iter=100000,\n",
    "                               C=100 #adjustable\n",
    "                              ).fit(X_train_balanced, y_train_balanced) # bigger C\n",
    "\n",
    "print(\"Training set score: {:.3f}\".format(logreg100.score(X_train_balanced, y_train_balanced)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg100.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.asarray([x for [x] in logreg100.coef_.T]).argsort()\n",
    "ranks= np.empty_like(temp)\n",
    "ranks[temp] = np.arange(len(temp))\n",
    "\n",
    "sorted_logreg100       = [0 for x in range(len(temp))]\n",
    "sorted_logreg          = [0 for x in range(len(temp))]\n",
    "sorted_logreg001       = [0 for x in range(len(temp))]\n",
    "sorted_logreg00001     = [0 for x in range(len(temp))]\n",
    "sorted_feature_columns = [0 for x in range(len(temp))]\n",
    "for i in range(len(temp)):\n",
    "    sorted_logreg100[ranks[i]]       = logreg100.coef_.T[i]\n",
    "    sorted_logreg[ranks[i]]          = logreg.coef_.T[i]\n",
    "    sorted_logreg001[ranks[i]]       = logreg001.coef_.T[i]\n",
    "    sorted_logreg00001[ranks[i]]     = logreg00001.coef_.T[i]\n",
    "    sorted_feature_columns[ranks[i]] = feature_columns[i]\n",
    "    \n",
    "# %% Plot: Compare LogReg-coefficients for different C (1, 100, 0.001)\n",
    "plt.figure(figsize=(70,10))\n",
    "plt.grid()\n",
    "plt.plot(sorted_logreg100 , '^', label=\"C=100\")\n",
    "plt.plot(sorted_logreg,    'o', label=\"C=1\")\n",
    "plt.plot(sorted_logreg001, 'v', label=\"C=0.001\")\n",
    "plt.plot(sorted_logreg00001, 's', label=\"C=0.0001\")\n",
    "plt.xticks(range(len(feature_columns)), sorted_feature_columns, rotation=90)\n",
    "plt.hlines(0, 0, len(feature_columns))\n",
    "plt.ylim(min(logreg100.coef_.T), max(logreg100.coef_.T))\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Coefficient magnitude\")\n",
    "plt.legend()\n",
    "plt.savefig('this_is_a_test.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Plot: Compare LogReg-coefficients for different C (1, 100, 0.001)\n",
    "plt.figure(figsize=(70,10))\n",
    "plt.grid()\n",
    "plt.plot(logreg100.coef_.T, '^', label=\"C=100\")\n",
    "plt.plot(logreg.coef_.T,    'o', label=\"C=1\")\n",
    "plt.plot(logreg001.coef_.T, 'v', label=\"C=0.001\")\n",
    "plt.plot(sorted_logreg00001, 's', label=\"C=0.0001\")\n",
    "plt.xticks(range(len(feature_columns)), feature_columns, rotation=90)\n",
    "plt.hlines(0, 0, len(feature_columns))\n",
    "plt.ylim(min(logreg.coef_.T), max(logreg.coef_.T))\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Coefficient magnitude\")\n",
    "plt.legend()\n",
    "plt.savefig('this_is_a_test.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lasso = Lasso().fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "print(\"Training set score: {:.2f}\".format(lasso.score(X_train_balanced, y_train_balanced)))\n",
    "print(\"Test set score: {:.2f}\".format(lasso.score(X_test, y_test)))\n",
    "print(\"Number of features used:\", np.sum(lasso.coef_ != 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lasso001 = Lasso(alpha=0.01, max_iter=100000).fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "print(\"Training set score: {:.2f}\".format(lasso001.score(X_train_balanced, y_train_balanced)))\n",
    "print(\"Test set score: {:.2f}\".format(lasso001.score(X_test, y_test)))\n",
    "print(\"Number of features used:\", np.sum(lasso001.coef_ != 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "lasso00000001 = Lasso(alpha=0.0000001, max_iter=100000).fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "print(\"Training set score: {:.2f}\".format(lasso00000001.score(X_train_balanced, y_train_balanced)))\n",
    "print(\"Test set score: {:.2f}\".format(lasso00000001.score(X_test,      y_test)))\n",
    "print(\"Number of features used:\", np.sum(lasso00000001.coef_ != 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lasso000001 = Lasso(alpha=0.0001, max_iter=100000).fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "print(\"Training set score: {:.2f}\".format(lasso000001.score(X_train_balanced, y_train_balanced)))\n",
    "print(\"Test set score: {:.2f}\".format(lasso000001.score(X_test, y_test)))\n",
    "print(\"Number of features used:\", np.sum(lasso000001.coef_ != 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=> ALL SUPERMASSIVE CRAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=10000,\n",
    "                                max_depth=15,\n",
    "                                random_state=42,\n",
    "                                n_jobs=-1)\n",
    "forest.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# %% Validate Accuracy\n",
    "print(\"Accuracy on training set: {:.3f}\".format(forest.score(X_train_balanced,y_train_balanced)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(forest.score(X_test,     y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Plot: Feature importance\n",
    "def plot_feature_importances(model,feature_columns,figsize=(20,100)):\n",
    "    from operator import itemgetter\n",
    "    dict_feature_importance = sorted(dict(zip(feature_columns,model.feature_importances_)).items(), key=itemgetter(1))\n",
    "    n_features              = len(feature_columns)\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.grid()\n",
    "    plt.barh(np.arange(n_features),\n",
    "             [y for (x,y) in dict_feature_importance],\n",
    "             align='center')\n",
    "    plt.yticks(np.arange(n_features), [x for (x,y) in dict_feature_importance])\n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    plt.ylim(-1, n_features)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importances(forest,feature_columns)\n",
    "#plot_feature_importances(forest_selected,feature_columns_selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Feature selection: SelectPercentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.feature_selection import f_classif # ANOVA F-value\n",
    "from sklearn.feature_selection import mutual_info_classif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``SelectPercentile`` removes all but a user-specified highest scoring percentage of features.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectPercentile.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ``f_classif `` ANOVA assumes normal distribution. (nope)\n",
    "2. ``mutual_info_classif`` Mutual information for a discrete target.\n",
    "3. ``chi2`` Chis-squared starts of non-negative features for classification tasks.\n",
    "4. ``f_regression`` F-value between label/feature for regression tasks.\n",
    "5. ``mutual_info_regression`` Mutual information for a continuous target.\n",
    "6. ``SelectKBest`` Select features based on the k highest scores\n",
    "7. ``SelectFpr`` Select features based on a FPR test.\n",
    "8. ``SelectFdr`` Select features based on an estimated false discovery rate\n",
    "9. ``SelectFwe`` Select features based on family-wise error rate.\n",
    "10. ``GenericUnivariateSelect`` Univariate feature selector with configurable mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# use f_classif (the default) and SelectPercentile to select 50% of features\n",
    "select = SelectPercentile(score_func=mutual_info_classif,\n",
    "                          percentile=50)\n",
    "select.fit(X_train_balanced, y_train_balanced)\n",
    "# transform training set\n",
    "X_train_selected = select.transform(X_train_balanced) # not needed. we have the mask (boolean array).\n",
    "\n",
    "print(\"X_train_balanced.shape: {}\".format(X_train_balanced.shape))\n",
    "print(\"X_train_selected.shape: {}\".format(X_train_selected.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = select.get_support()\n",
    "#print(mask)\n",
    "# visualize the mask -- black is True, white is False\n",
    "plt.matshow(mask.reshape(1, -1), cmap='gray_r')\n",
    "plt.xlabel(\"Sample index\")\n",
    "plt.yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import compress\n",
    "feature_columns_selected = list(compress(feature_columns,mask))\n",
    "\n",
    "print(\"Selected features:\\n\",feature_columns_selected)\n",
    "#print(feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train_balanced.shape: {}\".format(X_train_balanced.shape))\n",
    "print(\"X_train_selected.shape: {}\".format(X_train_selected.shape))\n",
    "print(\"X_train_balanced[:,mask].shape: {}\".format(X_train_balanced[:,mask].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "logreg_selected = LogisticRegression(n_jobs=-1,\n",
    "                            solver=\"sag\",\n",
    "                            C=1 # to be adjusted\n",
    "                           ).fit(X_train_balanced[:,mask], # We select only those we need\n",
    "                                 y_train_balanced)\n",
    "\n",
    "forest_selected = RandomForestClassifier(n_estimators=10000,\n",
    "                                max_depth=35,\n",
    "                                random_state=42,\n",
    "                                n_jobs=-1)\n",
    "\n",
    "forest_selected.fit(X_train_balanced[:,mask],\n",
    "           y_train_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Validate Accuracy\n",
    "print(\"Random Forest, newly selected features:\")\n",
    "print(\"Accuracy on training set: {:.3f}\".format(forest_selected.score(X_train_balanced[:,mask],\n",
    "                                                             y_train_balanced)))\n",
    "\n",
    "print(\"Accuracy on test set: {:.3f}\".format(forest_selected.score(X_test[:,mask],\n",
    "                                                         y_test)))\n",
    "\n",
    "print(\"\\nLogististic Regression C=1, newly selected features:\")\n",
    "print(\"Training set score: {:.3f}\".format(logreg_selected.score(X_train_balanced[:,mask], # Obviously only applicable to the subselect\n",
    "                                                                y_train_balanced))) \n",
    "print(\"Test set score: {:.3f}\".format(logreg_selected.score(X_test[:,mask], # Obviously only applicable to the subselect\n",
    "                                                            y_test)))\n",
    "\n",
    "print(\"\\nLogististic Regression C=1, all features:\")\n",
    "print(\"Training set score: {:.3f}\".format(logreg.score(X_train_balanced,y_train_balanced))) \n",
    "print(\"Test set score: {:.3f}\".format(logreg.score(X_test,y_test)))\n",
    "\n",
    "print(\"\\nRandom Forest, all features:\")\n",
    "print(\"Accuracy on training set: {:.3f}\".format(forest.score(X_train_balanced,\n",
    "                                                             y_train_balanced)))\n",
    "\n",
    "print(\"Accuracy on test set: {:.3f}\".format(forest.score(X_test,\n",
    "                                                         y_test)))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Plot: Compare LogReg-coefficients for different C (1, 100, 0.001)\n",
    "plt.figure(figsize=(70,10))\n",
    "plt.grid()\n",
    "plt.plot(logreg_selected.coef_.T,    'o', label=\"C=1\")\n",
    "plt.xticks(range(len(feature_columns_selected)), feature_columns_selected, rotation=90)\n",
    "plt.hlines(0, 0, len(feature_columns_selected))\n",
    "plt.ylim(min(logreg.coef_.T), max(logreg_selected.coef_.T))\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Coefficient magnitude\")\n",
    "plt.legend()\n",
    "plt.savefig('this_is_a_test.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importances(forest_selected,\n",
    "                         feature_columns_selected,\n",
    "                         figsize=(20,50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Model Validatian "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#logreg = LogisticRegression(C=1, solver='lbfgs').fit(X_train_binary, y_train_binary)\n",
    "pred_logreg          = logreg.predict(X_test)\n",
    "pred_logreg_selected = logreg_selected.predict(X_test[:,mask])\n",
    "pred_forest          = forest.predict(X_test)\n",
    "pred_forest_selected = forest_selected.predict(X_test[:,mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "confusion_logreg             = confusion_matrix(y_test, pred_logreg)\n",
    "df_confusion_logreg          = pd.DataFrame(confusion_logreg, index=[\"Fact 0\", \"Fact 1\"], columns=[\"Pred 0\",\"Pred 1\"])\n",
    "\n",
    "confusion_logreg_selected    = confusion_matrix(y_test, pred_logreg_selected)\n",
    "df_confusion_logreg_selected = pd.DataFrame(confusion_logreg_selected, index=[\"Fact 0\", \"Fact 1\"], columns=[\"Pred 0\",\"Pred 1\"])\n",
    "\n",
    "confusion_forest             = confusion_matrix(y_test, pred_forest)\n",
    "df_confusion_forest          = pd.DataFrame(confusion_forest, index=[\"Fact 0\", \"Fact 1\"], columns=[\"Pred 0\",\"Pred 1\"])\n",
    "\n",
    "confusion_forest_selected    = confusion_matrix(y_test, pred_forest_selected)\n",
    "df_confusion_forest_selected = pd.DataFrame(confusion_forest_selected, index=[\"Fact 0\", \"Fact 1\"], columns=[\"Pred 0\",\"Pred 1\"])\n",
    "\n",
    "print(\"Test set balance:\")\n",
    "print(sorted(Counter(y_test).items()),\"\\n\")\n",
    "\n",
    "print(\"Confusion Matrices:\")\n",
    "\n",
    "print(\"Logistic Regression C=1:\")\n",
    "display(df_confusion_logreg)\n",
    "\n",
    "print(\"Logistic Regression C=1, newly selected features:\")\n",
    "display(df_confusion_logreg_selected)\n",
    "\n",
    "print(\"Random Forest:\")\n",
    "display(df_confusion_forest)\n",
    "\n",
    "print(\"Random Forest, newly selected features:\")\n",
    "display(df_confusion_forest_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import mglearn\n",
    "#mglearn.plots.plot_binary_confusion_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $Precision =\\frac{TP}{TP+FP}$\n",
    "2. $Recall =\\frac{TP}{TP+FN}$\n",
    "3. $f_{1} = 2 \\cdot \\frac{precision \\cdot recall}{precision + recall} = \\frac{2 \\cdot TP}{2 \\cdot TP + (FN + FP)}$\n",
    "4. $Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}$ KPI combines Precision and Recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(\"Logistic Regression C=1:\")\n",
    "print(classification_report(y_test,\n",
    "                            pred_logreg,\n",
    "                            target_names=[\"not booking = 0\", \"booking = 1\"]))\n",
    "print(\"\\n\")\n",
    "print(\"Logistic Regression C=1, newly selected features:\")\n",
    "print(classification_report(y_test,\n",
    "                            pred_logreg_selected,\n",
    "                            target_names=[\"not booking = 0\", \"booking = 1\"]))\n",
    "print(\"\\n\")\n",
    "print(\"Random Forest:\")\n",
    "print(classification_report(y_test,\n",
    "                            pred_forest,\n",
    "                            target_names=[\"not booking = 0\", \"booking = 1\"]))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Random Forest, new selected:\")\n",
    "print(classification_report(y_test,\n",
    "                            pred_forest_selected,\n",
    "                            target_names=[\"not booking = 0\", \"booking = 1\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lower the Threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_test(number):\n",
    "    y_pred_lower_threshold = logreg.decision_function(X_test) > number\n",
    "    print(classification_report(y_test, y_pred_lower_threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_test(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_threshold(number):\n",
    "    y_pred_prob_threshold = np.asarray([y for [x,y] in logreg100.predict_proba(X_test)]) > number\n",
    "    print(classification_report(y_test, y_pred_prob_threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in np.around(np.arange(0,1,0.1),decimals=1):\n",
    "    print(\"Threshold =\",x,\":\")\n",
    "    print(prob_threshold(x),\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot_histogram(logreg.decision_function(X_test), \n",
    "                  #np.random.normal(loc=10, scale=5, size=10000), # gaussian distributed array, mean = 10, standard deviation = 5, length = 100000\n",
    "                  bins=200,\n",
    "                  figsize=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_test(-100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision-Recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "def prec_rec_curve(X_test,y_test):\n",
    "    global precision_logreg, recall_logreg, thresholds_logreg, precision_logreg_selected, recall_logreg_selected, thresholds_logreg_selected, precision_forest, recall_forest, thresholds_forest, precision_forest_selected, recall_forest_selected, thresholds_forest_selected\n",
    "    \n",
    "    precision_logreg, recall_logreg, thresholds_logreg = precision_recall_curve(\n",
    "        y_test, \n",
    "        logreg.decision_function(X_test))\n",
    "\n",
    "    #\n",
    "    precision_logreg_selected, recall_logreg_selected, thresholds_logreg_selected = precision_recall_curve(\n",
    "        y_test, \n",
    "        logreg_selected.decision_function(X_test[:,mask]))\n",
    "\n",
    "    # RandomForestClassifier has predict_proba, but not decision_function\n",
    "    precision_forest, recall_forest, thresholds_forest = precision_recall_curve(\n",
    "        y_test,\n",
    "        forest.predict_proba(X_test)[:, 1])\n",
    "\n",
    "    precision_forest_selected, recall_forest_selected, thresholds_forest_selected = precision_recall_curve(\n",
    "        y_test,\n",
    "        forest_selected.predict_proba(X_test[:,mask])[:, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#prec_rec_curve(X_test,y_test)\n",
    "prec_rec_curve(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,15))\n",
    "plt.grid()\n",
    "\n",
    "close_zero = np.argmin(np.abs(thresholds_logreg))\n",
    "plt.plot(precision_logreg[close_zero],\n",
    "         recall_logreg[close_zero],\n",
    "         'o',\n",
    "         markersize=10,\n",
    "         label=\"threshold zero LogReg\",\n",
    "         fillstyle=\"none\",\n",
    "         c='k',\n",
    "         mew=2)\n",
    "\n",
    "plt.plot(precision_forest, recall_forest, label=\"Random Forest\")\n",
    "plt.plot(precision_forest_selected, recall_forest_selected, label=\"Random Forest, newly selected features\")\n",
    "plt.plot(precision_logreg, recall_logreg, label=\"Logistic Regression, C=1\")\n",
    "plt.plot(precision_logreg_selected, recall_logreg_selected, label=\"Logistic Regression, C=1, newly selected features\")\n",
    "plt.xlabel(\"Precision\")\n",
    "plt.ylabel(\"Recall\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Remark:</b> This plot looks horrible. HORRIBLE!!!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.metrics import average_precision_score # Area under the Recall/Precision-curves\n",
    "\n",
    "avg_precision_forest = average_precision_score(y_test,\n",
    "                                               forest.predict_proba(X_test)[:, 1])\n",
    "\n",
    "avg_precision_forest_selected = average_precision_score(y_test,\n",
    "                                               forest_selected.predict_proba(X_test[:,mask])[:, 1])\n",
    "\n",
    "avg_precision_logreg = average_precision_score(y_test,\n",
    "                                               logreg.decision_function(X_test))\n",
    "\n",
    "avg_precision_logreg_selected = average_precision_score(y_test,\n",
    "                                               logreg_selected.decision_function(X_test[:,mask]))\n",
    "\n",
    "print(\"Average Precision of Random Forest: {:.3f}\".format(avg_precision_forest))\n",
    "print(\"Average Precision of Random Forest, newly selected features: {:.3f}\".format(avg_precision_forest_selected))\n",
    "print(\"Average Precision of Logistic Regression C=1: {:.3f}\".format(avg_precision_logreg))\n",
    "print(\"Average Precision of Logistic Regression C=1, newly selected features: {:.3f}\".format(avg_precision_logreg_selected))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Receiver operating characteristics (ROC) and AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $FPR = \\frac{FP}{FP + TN}$ \n",
    "2. $TPR = Recall$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr_logreg,          tpr_logreg,          thresholds_logreg      = roc_curve(y_test, logreg.decision_function(X_test))\n",
    "fpr_logreg00001,     tpr_logreg00001,     thresholds_logreg00001 = roc_curve(y_test, logreg00001.decision_function(X_test))\n",
    "fpr_logreg001,       tpr_logreg001,       thresholds_logreg001       = roc_curve(y_test, logreg001.decision_function(X_test))\n",
    "fpr_logreg100,       tpr_logreg100,       thresholds_logreg100       = roc_curve(y_test, logreg100.decision_function(X_test))\n",
    "fpr_logreg_selected, tpr_logreg_selected, thresholds_logreg_selected = roc_curve(y_test, logreg_selected.decision_function(X_test[:,mask]))\n",
    "fpr_forest,          tpr_forest,          thresholds_forest      = roc_curve(y_test, forest.predict_proba(X_test)[:, 1])\n",
    "fpr_forest_selected, tpr_forest_selected, thresholds_forest_selected = roc_curve(y_test, forest_selected.predict_proba(X_test[:,mask])[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.grid()\n",
    "\n",
    "plt.plot(fpr_logreg,fpr_logreg, linestyle='dotted', label=\"base line\")\n",
    "\n",
    "plt.plot(fpr_logreg,      tpr_logreg,      label=\"LogReg C=1\")\n",
    "#plt.plot(fpr_logreg00001, tpr_logreg00001, label=\"LogReg C=0.0001\")  # bad!\n",
    "plt.plot(fpr_logreg001,   tpr_logreg001,   label=\"LogReg C=0.01\")\n",
    "#plt.plot(fpr_logreg100,   tpr_logreg100,   label=\"LogReg C=100.0\")\n",
    "plt.plot(fpr_logreg_selected, tpr_logreg_selected,  label=\"LogReg C=1, selected\")\n",
    "plt.plot(fpr_forest,      tpr_forest,      label=\"ROC Curve Forest\")\n",
    "plt.plot(fpr_forest_selected,tpr_forest_selected,      label=\"ROC Curve Forest, selected\")\n",
    "\n",
    "\n",
    "plt.xlabel(\"False-Postive Rate (FPR)\")\n",
    "plt.ylabel(\"True-Positive Rate (TPR) aka. Recall\")\n",
    "\n",
    "# find threshold closest to zero\n",
    "close_zero_index = np.argmin(np.abs(thresholds_logreg))\n",
    "plt.plot(fpr_logreg[close_zero_index], tpr_logreg[close_zero_index],\n",
    "         'o',\n",
    "         markersize=10,\n",
    "         label=\"threshold close zero, LogReg C=1\",\n",
    "         fillstyle=\"none\",\n",
    "         c='k',\n",
    "         mew=2)\n",
    "\n",
    "close_default_index_forest = np.argmin(np.abs(thresholds_forest - 0.5))\n",
    "plt.plot(fpr_forest[close_default_index_forest], tpr_forest[close_default_index_forest],\n",
    "         '^',\n",
    "         markersize=10,\n",
    "         label=\"threshold 0.5 Random Forest\",\n",
    "         fillstyle=\"none\",\n",
    "         c='k',\n",
    "         mew=2)\n",
    "\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.metrics import roc_auc_score\n",
    "forest_auc = roc_auc_score(y_test, forest.predict_proba(X_test)[:, 1])\n",
    "forest_selected_auc = roc_auc_score(y_test, forest_selected.predict_proba(X_test[:,mask])[:, 1])\n",
    "\n",
    "logreg001_auc       = roc_auc_score(y_test, logreg001.decision_function(X_test))\n",
    "logreg_auc          = roc_auc_score(y_test, logreg.decision_function(X_test))\n",
    "logreg00001_auc     = roc_auc_score(y_test, logreg00001.decision_function(X_test))\n",
    "logreg100_auc       = roc_auc_score(y_test, logreg100.decision_function(X_test))\n",
    "logreg_selected_auc = roc_auc_score(y_test, logreg_selected.decision_function(X_test[:,mask]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AUC for Random Forest:           {:.3f}\".format(forest_auc))\n",
    "print(\"AUC for Random Forest, selected: {:.3f}\".format(forest_selected_auc))\n",
    "print(\"AUC for LogReg C=0.001:          {:.3f}\".format(logreg001_auc))\n",
    "print(\"AUC for LogReg C=0.0001:         {:.3f}\".format(logreg00001_auc))\n",
    "print(\"AUC for LogReg C=1, selected:    {:.3f}\".format(logreg_selected_auc))\n",
    "print(\"AUC for LogReg C=1:              {:.3f}\".format(logreg_auc))\n",
    "print(\"AUC for LogReg C=100:            {:.3f}\".format(logreg100_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation in Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch05.html#model-evaluation-and-improvement\n",
    "3. https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(logreg,\n",
    "                         np.concatenate([X_train_balanced,X_test]),\n",
    "                         np.concatenate([y_train_balanced,y_test]), \n",
    "                         cv=5)\n",
    "print(\"Cross-validation scores: {}\".format(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average cross-validation score: {:.2f}\".format(scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "res = cross_validate(logreg,\n",
    "                     np.concatenate([X_train_balanced,X_test]),\n",
    "                     np.concatenate([y_train_balanced,y_test]),\n",
    "                     cv=5,\n",
    "                     return_train_score=True)\n",
    "res_df = pd.DataFrame(res)\n",
    "display(res_df)\n",
    "print(\"Mean times and scores:\\n\", res_df.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
